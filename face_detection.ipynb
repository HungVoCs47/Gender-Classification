{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cd54ef0404a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCAMERA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCAPTURE_WIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBOTTOM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HungVo\\anaconda3\\lib\\site-packages\\imutils\\convenience.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, width, height, inter)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# grab the image size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# if both the width and height are None, then return the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "def preprocess(image, size = (64,64)):\n",
    "    labels = 0\n",
    "    label = tf.constant((labels), dtype = tf.int32)\n",
    "    image_cropped = tf.image.crop_to_bounding_box(image, 20, 0, 178, 178)\n",
    "    image_resized = tf.image.resize(image_cropped, size=size)\n",
    "    #image_resized =  image_resized[None, :, :]\n",
    "    image_resized = tf.expand_dims(image_resized, axis=0)\n",
    "    label1 = tf.expand_dims(label, axis=0)\n",
    "    #label = label[None,:]\n",
    "    return (image_resized/255,label1)\n",
    "\n",
    "\n",
    "model = keras.models.load_model('model.h5')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "# Load the cascade\n",
    "\n",
    "\n",
    "# To capture video from webcam. \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "CAPTURE_WIDTH = 900\n",
    "ROI_LONG = 178 # Region Of Interest\n",
    "MARGIN = 350\n",
    "TOP = 100\n",
    "RIGHT = CAPTURE_WIDTH - MARGIN\n",
    "BOTTOM = TOP + 218\n",
    "LEFT = RIGHT - ROI_LONG\n",
    "\n",
    "CAMERA = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    _, frame = CAMERA.read()\n",
    "    frame = imutils.resize(frame, CAPTURE_WIDTH)\n",
    "    (height, width) = frame.shape[:2]\n",
    "    cv2.rectangle(frame, (LEFT, TOP), (RIGHT, BOTTOM), (0,255,0), 2)\n",
    "    \n",
    "    img1 = frame[TOP:BOTTOM,LEFT:RIGHT]\n",
    "    img2 = preprocess(frame[TOP:BOTTOM,LEFT:RIGHT])\n",
    "    #print(img2)\n",
    "    #d = d.map(lambda x,y:tf.stack([x,y],axis=-1))\n",
    "    pred_logits = model.predict(img2[0])\n",
    "    probas = tf.sigmoid(pred_logits)\n",
    "    probas = probas.numpy().flatten()*100\n",
    "    \n",
    "    if probas >= 50:\n",
    "        pred = 'Male'\n",
    "    else:\n",
    "        pred = 'female'\n",
    "    LABEL_TEXT = str(pred)\n",
    "    LABEL_COLOR = (0,255,0)\n",
    "    cv2.putText(frame, LABEL_TEXT, (LEFT, TOP-7), cv2.FONT_HERSHEY_SIMPLEX, 1, LABEL_COLOR, 2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('frame1', img1)\n",
    "    # if the user pressed \"q\", then stop looping\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example, size = (64,64), mode = 'train'):\n",
    "    image = example['image']\n",
    "    #print(image.shape)\n",
    "    label = example['attributes']['Male']\n",
    "    #print(label)\n",
    "    if mode == 'train':\n",
    "        image_crop = tf.image.random_crop(image, size = (178, 178, 3))\n",
    "        image_resized = tf.image.resize(image_crop, size = size)\n",
    "        image_flip = tf.image.random_flip_left_right(image_resized)\n",
    "        return (image_flip/255, tf.cast(label, tf.int32))\n",
    "    else:\n",
    "        image_cropped = tf.image.crop_to_bounding_box(\n",
    "            image, 0, 0, 178, 178)\n",
    "        image_resized = tf.image.resize(\n",
    "            image_cropped, size=size)\n",
    "        print(image_resized/255.0, tf.cast(label, tf.int32))\n",
    "        return (image_resized/255.0, tf.cast(label, tf.int32))\n",
    "    \n",
    "import tensorflow_datasets as tfds\n",
    "dataset = tfds.builder('celeb_a')\n",
    "dataset.download_and_prepare()\n",
    "data = dataset.as_dataset(shuffle_files=False)\n",
    "data_test = data['test']\n",
    "ds_test = data_test.map(\n",
    "    lambda x:preprocess(x, mode='eval')).batch(32)\n",
    "results = model.evaluate(ds_test, verbose=0)\n",
    "print('Test Acc: {:.2f}%'.format(results[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07f346ca2ec33ab218d39bbb959bc4c656ffa155dfb5e9f856f2cf5c91d54524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
